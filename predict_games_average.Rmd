---
title: "Predicting Average Ratings"
author: Phil Henrickson
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: true
---

This notebook is for building predictive models of boardgame ratings.

```{r global seetings, echo=F, warning=F, message=F}

knitr::opts_chunk$set(echo = F,
                      dev="png",
                      fig.width = 8,
                      fig.height = 8)

options(knitr.duplicate.label = "allow")

options(scipen=999)

```

```{r load and set packages, warning=F, message=F, include=FALSE, results = 'hide'}

source("load_packages.R")
source("theme_phil.R")

```

## Get Data from Big Query

### Active Game Rankings

We'll first connect to the most recent day of BGG data that we have in our database. These are the active rankings of games - where they stand in the BGG database as of the most recent load, which I usually update once a week.

```{r connect to big query}

library(bigrquery)

# get project credentials
PROJECT_ID <- "gcp-analytics-326219"
BUCKET_NAME <- "test-bucket"


# establish connection
bigquerycon<-dbConnect(
        bigrquery::bigquery(),
        project = PROJECT_ID,
        dataset = "bgg"
)

```


```{r query active games}

# query table
active_games<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT * FROM bgg.active_games_daily')

```

### Additional Game Information

We also want to pull down other tables containing the information that we know about games.

```{r query tables with game information}

# general game info
games_info<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT * FROM bgg.active_games_info')

# game categories
game_categories<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT 
                              a.game_id,
                              b.category_id,
                              b.category
                              FROM bgg.game_categories a
                               LEFT JOIN bgg.category_ids b 
                               ON a.category_id = b.category_id')

# game mechanics
game_mechanics<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT 
                              a.game_id,
                              b.mechanic_id,
                              b.mechanic
                              FROM bgg.game_mechanics a
                               LEFT JOIN bgg.mechanic_ids b 
                               ON a.mechanic_id = b.mechanic_id')

# game publishers
game_publishers<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT 
                              a.game_id,
                              b.publisher_id,
                              b.publisher
                              FROM bgg.game_publishers a
                               LEFT JOIN bgg.publisher_ids b 
                               ON a.publisher_id = b.publisher_id')

# game designers
game_designers<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT 
                              a.game_id,
                              b.designer_id,
                              b.designer
                              FROM bgg.game_designers a
                               LEFT JOIN bgg.designer_ids b 
                               ON a.designer_id = b.designer_id')

# # game artists
# game_artists<-DBI::dbGetQuery(bigquerycon, 
#                               'SELECT 
#                               a.game_id,
#                               b.artist_id,
#                               b.artist
#                               FROM bgg.game_artists a
#                                LEFT JOIN bgg.artist_ids b 
#                                ON a.artist_id = b.artist_id')


```

## Exploratory Analysis

We now will explore how game ratings ratings are a function of the features we have in the dataset. At this stage in the process, we want to restrict ourselves to looking at our training set and to get a basic understanding of both our outcome variable and the features.

### Features

#### Users Rated

Users rated affects the average, in what is likely something of a positive feedback loop. I won't be using this feature in modeling, as it is a quantity we will need to forecast.

```{r users rated vs average}

active_games %>% 
        filter(usersrated > 200) %>%
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  ggplot(., aes(x=log(usersrated), 
                label = name,
                y=average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  theme(legend.title = element_text())+
  guides(size = guide_legend(title = "Users Rated",
                             title.position = "top"),
         color = "none")+
  xlab("Users Rated (logged)")+
  ylab("Average Rating")+
        geom_smooth(method = 'loess', formula = 'y ~ x')


```


#### Year Published

The minimum year published in our dataset is -3000, which are games like Marbles and Backgammon. If we filter to games from 1950 to 2022, we can see the rise in the Average Rating of newer games.

```{r plot game ratings vs year published, warning=F}

#library(plotly)
# filter to games publisehd after 19gg50
#ggplotly(

active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  ggplot(., aes(x=yearpublished, 
                label = name,
                y=average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  theme(legend.title = element_text())+
  guides(size = guide_legend(title = "Users Rated",
                             title.position = "top"),
         color = "none")+
  xlab("Year Published")+
  ylab("Average Rating")+
        geom_smooth(method = 'loess', 
                    formula = 'y ~ x')

```

#### Weight, Playing Time, and Min/Max Players

We know that there's a strong correlation between the complexity of a game and its Average Rating, as the BGG community tends to buy and like heavier games.

```{r plot these other two, warning=F, message=F}

# hist
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  ggplot(., aes(x=avgweight))+
  geom_density(fill = 'grey20',
               alpha=0.8,
               color = NA)+
  theme_phil()+
  xlab("Average Weight")

# plot vs bgg rating
# scatter
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  ggplot(., aes(x=avgweight, 
                label = name,
                y=average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  ylab("Average Rating")+
  xlab("Average Weight")

```

Playing time is an interesting feature of a game that I would expect to have a nonlinear relationship with the Average Rating. 

```{r plot playing time, warning=F, message=F}

# summary
summary(active_games$playingtime)

# which are the longest games?
active_games %>% 
  arrange(desc(playingtime)) %>%
  select(game_id, name, playingtime, avgweight, average) %>%
  mutate_if(is.numeric, round, 3) %>%
  head(100) %>%
  flextable() %>%
  autofit()

# hist
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  ggplot(., aes(x=log(playingtime)))+
  geom_density(fill = 'grey20',
               alpha=0.8,
               color = NA)+
  theme_phil()+
  xlab("Playing Time in Logged Minutes")+
  geom_vline(xintercept = log(c(15, 60, 120, 360)),
             linetype = 'dotted')+
  labs(caption = "Lines at 15, 60, 120, and 360, respectively")

# plot vs bgg rating
# scatter
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  mutate(playingtime = log1p(playingtime)) %>%
  ggplot(., aes(x=playingtime,
                label = name,
                y=average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  ylab("Average Rating")+
  xlab("Playing Time in Minutes (logged)")+
  geom_vline(xintercept = log1p(c(15, 60, 120, 360)),
             linetype = 'dotted')+
  geom_smooth()+
  labs(caption = "Lines at 15, 60, 120, and 360, respectively")


# plot vs bgg rating
# scatter
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2022) %>%
  filter(playingtime > 0 & playingtime < 600) %>%
  ggplot(., aes(x=playingtime,
                label = name,
                y=average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  ylab("Average Rating")+
  xlab("Playing Time in Minutes")+
  geom_smooth()


```
Min and max players

```{r now look at min and max players}

# scatter min players
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2021) %>%
  ggplot(., aes(x=minplayers,
                y = average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  geom_smooth()+
  xlab("Minimum Players")+
  ylab("Average Rating")

# scatter min players
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2021) %>%
  ggplot(., aes(x=log1p(minplayers),
                y = average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  geom_smooth()+
  xlab("Minimum Players (logged)")+
  ylab("Average Rating")


# scatter max players
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2021) %>%
  ggplot(., aes(x=maxplayers,
                y = average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  geom_smooth()+
  xlab("Maximum Players")+
  ylab("Average Rating")

# what are theese games with a ton of players
active_games %>% 
  arrange(desc(maxplayers)) %>%
  select(game_id, name, minplayers, maxplayers, avgweight, average) %>%
  mutate_if(is.numeric, round, 3) %>%
  head(100) %>%
  flextable() %>%
  autofit()
  
# 
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2021) %>%
  ggplot(., aes(x=log1p(maxplayers),
                y = average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  geom_smooth()+
  xlab("Maximum Players (logged)")+
  ylab("Average Rating")

# 
active_games %>% 
  filter(yearpublished > 1950) %>%
  filter(yearpublished < 2021) %>%
  mutate(maxplayers_trunc = case_when(maxplayers >= 12 ~ 12,
                   TRUE ~ maxplayers)) %>%
  arrange(desc(maxplayers_trunc)) %>%
  ggplot(., aes(x=maxplayers_trunc,
                y = average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  geom_smooth()+
  xlab("Maximum Players Truncated")+
  ylab("Average Rating")

```

What about playing time per player?

```{r playing time per player, warning=F, message=F}

# distribution
active_games %>%
  select(game_id, name, average, playingtime, minplaytime, maxplaytime, minplayers, maxplayers) %>%
  arrange(game_id) %>%
  mutate(minplayers = case_when(minplayers == 0 ~ 1,
                                TRUE ~ minplayers)) %>%
  mutate(maxplayers = case_when(maxplayers == 0 ~ minplayers,
                                TRUE ~ maxplayers)) %>%
  mutate(time_per_player = maxplaytime / maxplayers) %>%
  ggplot(., aes(x=log1p(time_per_player)))+
  geom_density(fill = 'grey20',
               color =NA,
               alpha=0.8)+
  theme_phil()+
  xlab("Time Per Player (logged)")

# scatter
active_games %>%
  select(game_id, name, average, playingtime, minplaytime, maxplaytime, minplayers, maxplayers) %>%
  arrange(game_id) %>%
  mutate(minplayers = case_when(minplayers == 0 ~ 1,
                                TRUE ~ minplayers)) %>%
  mutate(maxplayers = case_when(maxplayers == 0 ~ minplayers,
                                TRUE ~ maxplayers)) %>%
  mutate(time_per_player = maxplaytime / maxplayers) %>%
  ggplot(., aes(x=log1p(time_per_player),
                y=average))+
  geom_jitter(alpha=0.5)+
  theme_phil()+
  geom_smooth(method = 'loess', formula = 'y ~ x')+
  xlab("Playing Time per Player (logged)")+
  ylab("Average Rating")

```

#### Game Ratings by Category

```{r explore ratings by category, fig.height=4}

# filter to games with publishers that had at least 300 games
# jitter plot
active_games %>%
  filter(yearpublished < 2022) %>%
  left_join(., game_categories,
            by = "game_id") %>%
  select(timestamp, game_id, name, category_id, category, everything()) %>%
  filter(!is.na(category)) %>%
  #filter(category_id %in% top_categorys$category_id) %>%
  filter(yearpublished<2021) %>%
  group_by(category_id) %>%
  mutate(median_rating = median(average),
         n_games = n_distinct(game_id)) %>%
  ungroup() %>%
  filter(n_games > 200) %>%
  ggplot(., aes(x=reorder(category, 
                          median_rating),
                y=average))+
  geom_jitter(width=0.2,
              height=0,
              alpha=0.25)+
  coord_flip(ylim = c(4, 8.5))+
  theme_phil()+
  geom_boxplot(alpha=0.75)+
  xlab("Game Category")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Game Category",
          subtitle = str_wrap("Displaying Average Ratings for all games published prior to 2022 for categories with at least 200 games. Points jittered to improve visibility.", 125))+
  labs(caption=paste("Data from boardgamegeek.com as of", max(as.Date(active_games$timestamp))))+
  theme(plot.title = element_text(size=12),
        plot.subtitle =  element_text(size=10),
        axis.text.y = element_text(size=8))

```

#### Game Ratings by Mechanic

Look at ratings by mechanic.

```{r table of top mechanics}

top_mechanics<-active_games %>%
  filter(yearpublished<2022) %>%
  left_join(., game_mechanics,
            by = "game_id") %>%
  select(timestamp, game_id, name, mechanic_id, mechanic, everything()) %>%
  filter(!is.na(mechanic)) %>%
  group_by(mechanic_id, mechanic) %>%
  summarize(games = n_distinct(game_id),
            median_rating = median(average, na.rm=T),
            sd_rating = sd(average, na.rm=T),
            .groups = 'drop') %>%
  ungroup() %>%
  filter(games > 10) %>%
  arrange(desc(median_rating)) %>%
  arrange(desc(games))

top_mechanics %>%
    mutate_if(is.numeric, round ,2) %>%
    rename(`Mechanic ID` = mechanic_id,
         `Mechanic` = mechanic,
         `# Games` = games,
         `Median Rating` = median_rating,
         `SD Rating` = sd_rating) %>%
  arrange(desc(`Median Rating`)) %>%
  DT::datatable()

rm(top_mechanics)
  
```

Plot the distribution by mechanic, filtering to games with only a set number.

```{r plot by mechanics, fig.height=4}

# filter to games with publishers that had at least 300 games
active_games %>%
  left_join(., game_mechanics,
            by = "game_id") %>%
  select(timestamp, game_id, name, mechanic_id, mechanic, everything()) %>%
  filter(!is.na(mechanic)) %>%
  #filter(mechanic_id %in% top_mechanics$mechanic_id) %>%
  filter(yearpublished<2022) %>%
  group_by(mechanic_id) %>%
  mutate(median_rating = median(average),
         n_games = n_distinct(game_id)) %>%
  ungroup() %>%
  filter(n_games > 200) %>%
  ggplot(., aes(x=reorder(mechanic, 
                          median_rating),
                y=average))+
  geom_jitter(width=0.2,
              height=0,
              alpha=0.25)+
  coord_flip(ylim = c(4, 8.5))+
  theme_phil()+
  geom_boxplot(alpha=0.75)+
  xlab("Game Mechanic")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Game Mechanic",
          subtitle = str_wrap("Displaying Average Ratings for all games published prior to 2022 for mechanics with at least 200 games", 125))+
  labs(caption=paste("Data from boardgamegeek.com as of", max(as.Date(active_games$timestamp))))+
  theme(plot.title = element_text(size=12),
        plot.subtitle =  element_text(size=10),
        axis.text.y = element_text(size=8))



```
```{r designer ratings table}

top_designers<-active_games %>%
  filter(yearpublished<2021) %>%
  left_join(., game_designers,
            by = "game_id") %>%
  select(timestamp, game_id, name, designer_id, designer, everything()) %>%
  mutate(designer = gsub("\\)", "", gsub("\\(", "", designer))) %>%
  filter(!is.na(designer)) %>%
  group_by(designer_id, designer) %>%
  summarize(games = n_distinct(game_id),
            median_rating = median(average, na.rm=T),
            sd_rating = sd(average, na.rm=T),
            .groups = 'drop') %>%
  ungroup() %>%
  arrange(desc(median_rating)) %>%
  filter(games > 5) %>%
  arrange(desc(median_rating)) %>%
  mutate_if(is.numeric, round, 3)

top_designers %>%
  rename(`Designer ID` = designer_id,
         `Designer` = designer,
         `Published Games` = games,
         `Median Rating` = median_rating,
         `SD Rating` = sd_rating) %>%
  DT::datatable()

```

We can visualize each designer's distribution of games.

```{r visualize designer games, fig.height=4}

active_games %>%
  left_join(., game_designers,
            by = "game_id") %>%
  select(timestamp, game_id, name, designer_id, designer, everything()) %>%
  filter(!is.na(designer)) %>%
  #filter(designer_id %in% top_designers$designer_id) %>%
  filter(yearpublished<2021) %>%
  group_by(designer_id) %>%
  mutate(median_rating = median(average),
         n_games = n_distinct(game_id)) %>%
  ungroup() %>%
  filter(n_games > 25) %>%
  ggplot(., aes(x=reorder(designer, 
                          median_rating),
                y=average))+
  geom_point(alpha=0.25)+
  # geom_jitter(width=0.15,
  #             height=0,
  #             alpha=0.25)+
  coord_flip(ylim = c(4, 8.5))+
  theme_phil()+
  geom_boxplot(alpha=0.75)+
  xlab("Game Designer")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Game Designer",
          subtitle = str_wrap("Displaying Average Ratings for all games published prior to 2021 for game designers with at least 25 games", 125))+
  labs(caption=paste("Data from boardgamegeek.com as of", max(as.Date(active_games$timestamp))))+
  theme(plot.title = element_text(size=12),
        plot.subtitle =  element_text(size=10),
        axis.text.y = element_text(size=6))

rm(top_designers)

```

### Publisher

```{r publisher, fig.height=4}

active_games %>%
  left_join(., game_publishers,
            by = "game_id") %>%
  select(timestamp, game_id, name, publisher_id, publisher, everything()) %>%
  filter(!is.na(publisher)) %>%
  #filter(publisher_id %in% top_publishers$publisher_id) %>%
  filter(yearpublished<2021) %>%
  group_by(publisher_id) %>%
  mutate(median_rating = median(average),
         n_games = n_distinct(game_id)) %>%
  ungroup() %>%
  filter(n_games > 100) %>%
  ggplot(., aes(x=reorder(publisher, 
                          median_rating),
                y=average))+
  geom_point(alpha=0.25)+
  # geom_jitter(width=0.15,
  #             height=0,
  #             alpha=0.25)+
  coord_flip(ylim = c(4, 8.5))+
  theme_phil()+
  geom_boxplot(alpha=0.75)+
  xlab("Game publisher")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Game Publisher",
          subtitle = str_wrap("Displaying Average Ratings for all games published prior to 2021 for game publishers with at least 100 games", 125))+
  labs(caption=paste("Data from boardgamegeek.com as of", max(as.Date(active_games$timestamp))))+
  theme(plot.title = element_text(size=12),
        plot.subtitle =  element_text(size=10),
        axis.text.y = element_text(size=6))

```

Let's create a lsit of verified publishers.

```{r list of publishers}

game_publishers %>% 
        group_by(publisher_id, publisher) %>% 
        summarize(games = n_distinct(game_id)) %>% arrange(desc(games))

publisher_list = c(51,
                   4,
                   157,
                   34,
                   28,
                   10001,
                   39,
                   37,
                   20,
                   3,
                   538,
                   52,
                   8923,
                   17,
                   5,
                   3320,
                   597,
                   5400,
                   26,
                   47,
                   2726,
                   11652,
                   19,
                   13,
                   12024,
                   28072)




```


## Predictive Modeling

Let's now turn to the task of predictive modeling.

Our outcome is the Average Rating, which is a function of the games average + the number of user ratings.


```{r plot games and user ratings, warning=F, message=F}

active_games %>%
              filter(yearpublished < 2022) %>%
              mutate(min_user_ratings = 100) %>%
              nest(-min_user_ratings) %>%
  bind_rows(.,
            active_games %>%
              filter(yearpublished < 2022) %>%
              mutate(min_user_ratings = 250) %>%
              nest(-min_user_ratings)) %>%
    bind_rows(.,
            active_games %>%
              filter(yearpublished < 2022) %>%
              mutate(min_user_ratings = 1000) %>%
              nest(-min_user_ratings)) %>%
  unnest() %>%
  filter(usersrated < min_user_ratings) %>%
  select(min_user_ratings, usersrated, average) %>%
  mutate(min_user_ratings = factor(min_user_ratings)) %>%
  melt(id.vars=c("min_user_ratings")) %>%
  ggplot(., aes(x=value,
                fill = min_user_ratings,
                color = min_user_ratings)) +
  geom_density(alpha=0.8)+
  facet_wrap(variable~.,
             scales="free",
             ncol =1)+
  theme_phil()+
  scale_fill_viridis_d()+
  scale_color_viridis_d()


```

### Create Game Dataset

```{r pivot and join}

min_users = 200

# combine all
games_train<-active_games %>%
  select(timestamp, game_id, name, average, baverage, usersrated) %>%
  filter(usersrated > min_users) %>%
  left_join(., games_info %>% # join game info
              select(game_id, yearpublished, avgweight, minage, minplayers, maxplayers, playingtime),
            by = c("game_id")) %>%
  filter(yearpublished < 2020) %>% # use games prior to 2022 as our training set
  left_join(., game_categories %>% # join categories
              mutate(category = gsub("\\)", "", gsub("\\(", "", category))) %>%
              mutate(category = tolower(paste("cat", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", category))), sep="_"))) %>%
              mutate(has_category = 1) %>%
              select(-category_id) %>%
              pivot_wider(names_from = c("category"),
                          values_from = c("has_category"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
            by = c("game_id")) %>%
  left_join(., game_mechanics %>% # join mechanics
              mutate(mechanic = tolower(paste("mech", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", mechanic))), sep="_"))) %>%
              mutate(has_mechanic = 1) %>%
              select(-mechanic_id) %>%
              pivot_wider(names_from = c("mechanic"),
                          values_from = c("has_mechanic"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
            by = c("game_id")) %>%
      left_join(., game_designers %>% # join designers
              mutate(designer = gsub("\\)", "", gsub("\\(", "", designer))) %>%
              mutate(designer = tolower(paste("des", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", designer))), sep="_"))) %>%
              mutate(has_designer = 1) %>%
              select(-designer_id) %>%
              pivot_wider(names_from = c("designer"),
                          values_from = c("has_designer"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
            by = c("game_id")) %>%
        left_join(., game_publishers %>% # join publishers
                          filter(publisher_id %in% publisher_list) %>%
              mutate(publisher = gsub("\\)", "", gsub("\\(", "", publisher))) %>%
              mutate(publisher = tolower(paste("pub", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", publisher))), sep="_"))) %>%
              mutate(has_publisher = 1) %>%
              select(-publisher_id) %>%
              pivot_wider(names_from = c("publisher"),
                          values_from = c("has_publisher"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
              by = c("game_id"))

```

###  Predicting Average Rating

We'll create a recipe for our baseline model, which only uses observable game info such as playing time, player counts, complexity (more on this later), and categories + mechanics.

A couple of things to note here. We're going to filter to games published since 1950, as well as filter to games with a set number of user ratings.

```{r create a recipe for the Average Rating}

recipe_average<- recipe(average~., x = games_train) %>%
          update_role(timestamp,
                usersrated,
                game_id,
                name,
                baverage,
                new_role = "id") %>%
          step_filter(!is.na(yearpublished)) %>%
          step_filter(
              cat_collectible_components !=1 &
              cat_expansion_for_basegame != 1) %>% # remove specific categories that count expansions
         # step_filter(yearpublished > 1950) %>%
         step_mutate(published_prior_1900 = case_when(yearpublished<1900 ~ 1,
                                                             TRUE ~ 0)) %>%
          step_mutate(yearpublished = case_when(yearpublished <= 1900 ~ 1900,
                                                 TRUE ~ as.numeric(yearpublished))) %>% # truncate yearpublished
         # step_mutate(years_since_published = as.numeric(year(Sys.Date()))-yearpublished) %>%
          step_impute_median(avgweight,
                            minplayers,
                            maxplayers,
                            playingtime,
                            minage) %>% # medianimpute numeric predictors
          step_mutate(minplayers = case_when(minplayers < 1 ~ 1,
                                             minplayers > 10 ~ 10,
                                             TRUE ~ minplayers),
                      maxplayers = case_when(maxplayers < 1 ~ minplayers,
                                             maxplayers > 20 ~ 20,
                                             TRUE ~ maxplayers)) %>% # truncate player range
          step_mutate(time_per_player = playingtime/ maxplayers) %>% # make time per player variable
          step_mutate_at(starts_with("cat_"),
                   fn = ~ replace_na(., 0)) %>%
          step_mutate_at(starts_with("mech_"),
                   fn = ~ replace_na(., 0)) %>%
          step_mutate_at(starts_with("des_"),
                   fn = ~ replace_na(., 0)) %>%
        step_mutate_at(starts_with("pub_"),
                   fn = ~ replace_na(., 0)) %>%
          step_mutate(number_mechanics = rowSums(across(starts_with("mech_"))),
                      number_categories = rowSums(across(starts_with("cat_"))),
                      number_designers = rowSums(across(starts_with("des_")))) %>%
          step_log(playingtime,
                   usersrated,
             time_per_player,
             offset = 1) %>%
          step_zv(all_predictors()) %>%
          step_nzv(all_predictors(),
             freq_cut = 150/1) %>%
          check_missing(all_numeric_predictors())

# normalize
recipe_norm_average<-recipe_average %>%
  step_normalize(all_predictors())

# summary of recipe
summary(recipe_average)

```

#### Baseline Error Rate

Before we get into modeling, let's establish up front the baseline accuracy of a null model. If we were to simply predict the mean for every game, how well would we do?

```{r establish baseline, warning=F}

# specify regression metrics
reg_metrics<-metric_set(yardstick::rmse,
                        yardstick::rsq,
                        yardstick::mae,
                        yardstick::mape)

#  get performance of baseline
null_results<-games_train %>%
  select(average) %>%
  mutate(pred = mean(average)) %>%
  reg_metrics(truth = average,
              estimate = pred) %>%
  filter(!is.na(.estimate)) %>%
  mutate_if(is.numeric, round, 3) %>%
  mutate(model = "null") %>%
  select(model, everything())

null_results

```

#### Worfklow with tidymodels

Now that we have our recipe in place, we can proceed to building a modeling workflow. We'll set up cross validation on our training set for tuning models which rely on tuning parameters.

```{r set up validation set, warning=F, message=F}
library(rsample)

set.seed(234)
train_folds<-vfold_cv(games_train, 
                      strata = average,
                      v = 5,
                      repeats = 1)

```

Now we'll set up the models we plan to use, which to start will be a penalized linear regression, a linear regression fit with Stan, and gradient boosted trees.

```{r specify models}

library(tidymodels)
library(workflows)

# simple linear regression
lm_mod <- 
  linear_reg() %>% 
  set_engine("lm")

# stan linear regression
set.seed(123)
prior_dist <- rstanarm::student_t(df = 1)

stan_lm_mod <-   
  linear_reg() %>% 
  set_engine("stan", 
             prior_intercept = prior_dist, 
             prior = prior_dist,
             iter = 6000)

# penalized linear regression
glmnet_mod<- 
  linear_reg(penalty = tune::tune(),
             mixture = 0.5) %>%
  set_engine("glmnet")

```

##### Penalized Regression

Now build the workflow and train the model.

```{r set up workflow}

# specify grid for tuning
glmnet_grid <- tibble(penalty = 10^seq(-4, -1, 
                                       length.out = 30))

# build workflow
set.seed(1999)
glmnet_workflow<-
  workflow() %>% 
  add_model(glmnet_mod) %>% 
  add_recipe(recipe_norm_average %>%
               step_normalize(all_predictors()))

# tune
glmnet_results <-
  glmnet_workflow %>%
  tune_grid(train_folds,
            grid = glmnet_grid,
            control = control_grid(save_pred = TRUE),
            metrics = reg_metrics) %>%
  mutate(model = "glmnet")

```

Let's look at the results across the tuning range.

```{r show tuning range for glmnet}

# plot
glmnet_results %>%
  collect_metrics() %>%
  ggplot(., aes(x=penalty,
                y=mean))+
  geom_line()+
  facet_wrap(model~.metric,
             scales="free_y")+
  theme_phil()

```

We can plot the predicted vs actual.

```{r plot predictions glmnet}
 
# grab the best tuning results
glmnet_best<-glmnet_results %>%
  select_best("rmse")

# what were our best results
glmnet_results %>% 
  collect_metrics() %>%
  filter(penalty == glmnet_best$penalty) %>%
  mutate_if(is.numeric, round, 3) %>%
  select(model, .metric, mean, std_err)
  
# grab the predictions and plot
glmnet_results %>%
  collect_predictions(parameters = glmnet_best) %>%
  mutate(model = "glmnet") %>%
  ggplot(., aes(x=.pred,
                y=average))+
  geom_point(alpha=0.5)+
  facet_wrap(model~.)+
  theme_phil()+
  geom_smooth(formula = y ~ x, method = "loess")+
  coord_cartesian(xlim = c(3.5, 9),
                  ylim = c(3.5, 9))+
  stat_cor(label.x = 3.5,
           p.accuracy = 0.001)+
  geom_abline(slope =1,
              intercept = 0,
              col = "grey60",
              linetype = 'dashed')

```

And a residual plot.

```{r residual plot from glmnet}

glmnet_results %>%
  collect_predictions(parameters = glmnet_best) %>%
  mutate(model = "glmnet") %>%
  select(id, .row, average, .pred, penalty, model) %>%
  mutate(.resid = average - .pred) %>%
  ggplot(., aes(x=.pred, 
                y= .resid))+
  geom_point(alpha=0.5)+
  theme_phil()+
  facet_wrap(model~.)
  
```

I'm interested in plot to see how the predictions compare to the actual in terms of their rank. It's less important that we get the magnitude right than that we get the ranking.

```{r percentile percentile plot, warning=F, message=F}

glmnet_results %>%
  collect_predictions(parameters = glmnet_best) %>%
  mutate(model = "glmnet") %>%
  select(id, .row, average, .pred, penalty, model) %>%
  melt(id.vars = c("id", ".row", "penalty", "model")) %>%
  group_by(variable) %>%
  summarise(quantile = seq(0, 1, 0.025),
            value = quantile(value, seq(0, 1, 0.025)),
            .groups = 'drop') %>%
  ggplot(., aes(x=quantile,
                y=value,
                colour = variable))+
  geom_point()+
  theme_phil()+
  scale_color_manual(values = c("deepskyblue1", "navy"))

glmnet_results %>%
  collect_predictions(parameters = glmnet_best) %>%
  sample_n(500) %>%
  mutate(model = "glmnet") %>%
  select(id, .row, average, .pred, penalty, model) %>%
  arrange(average) %>%
  mutate(rank = row_number()) %>%
  melt(id.vars = c("id", ".row", "penalty", "model", "rank")) %>%
  ggplot(., aes(x=rank,
                y = value,
                color = variable))+
  geom_point(alpha=0.75)+
  theme_phil()+
  scale_color_manual(values = c("deepskyblue1", "navy"))+
  facet_wrap(model~.)+
  ylab("Rating")+
  xlab("Rank")

```

Let's take a look at the individual predictions.

```{r get predictions from glmnet, warning=F, message=F}

# baked data
baked_train<- recipe_average %>%
  prep(games_train, strings_as_factor = F) %>%
  bake(games_train) %>%
  mutate(.row = row_number())

# set color functions
col_func<- function(x) {
  
  breaks<-quantile(x, probs = seq(0, 1, by=.1), na.rm=T) %>% as.vector()
#  breaks = weight_deciles
  colorRamp=colorRampPalette(c("red", "white", "deepskyblue1"))
  col_palette <- colorRamp(length(breaks))
  mycut <- cut(x, 
    breaks = breaks,
    include.lowest = TRUE, 
    right=T,
    label = FALSE)
  col_palette[mycut]
  
}

set.seed(1999)
samp<-baked_train %>%
  mutate(strata = plyr::round_any(average, .5)) %>%
  select(game_id, strata) %>%
  group_by(strata) %>%
  filter(strata > 5 & strata <8.5) %>%
  sample_n(size=8) %>%
  pull(game_id)

# get predictions
glmnet_results %>%
  collect_predictions(parameters = glmnet_best) %>%
  left_join(., baked_train) %>%
  arrange(.row) %>%
  select(id, .pred, .row, average, game_id, name) %>%
  filter(game_id %in% samp) %>%
  arrange(desc(.pred)) %>%
  mutate(game_id = as.character(game_id)) %>%
  mutate(model = "glmnet") %>%
  select(model, game_id, name, average, .pred) %>%
  arrange(desc(average)) %>%
  mutate_if(is.numeric, round, 3) %>%
  flextable() %>%
  autofit() %>%
        bg(j = c('.pred', 'average'),
           bg = col_func)


  # ggplot(., aes(x=.pred,
  #               label = name,
  #               y= average))+
  # geom_point()+
  # geom_label_repel(max.overlaps=10,
  #                  size = 2.5)+
  # theme_phil()+
  # geom_abline(slope =1,
  #        intercept=0,
  #        linetype = 'dashed')

glmnet_fit<-glmnet_workflow %>%
  finalize_workflow(glmnet_best) %>%
  fit(games_train)

glmnet_coefs<-glmnet_fit %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  filter(term != '(Intercept)')

```

The model completely whiffs on Crokinole! I'm not really surprised, given that it has very few things that the model really likes such as complexity, mechanics, as well as recency, which we can see when we unpack the coefficients from the model.

```{r show the damn coef plots, fig.height=4}

glmnet_coefs<-glmnet_workflow %>%
  finalize_workflow(glmnet_best) %>%
  fit(games_train) %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  filter(term != '(Intercept)')


rename_func<-function(x) {
  
  x<-gsub("cat_memory", "cat_memory_game", x)
  x<-gsub("cat_","", x)
  x<-gsub("mech_","", x)
  x<-gsub("pub_","", x)
  x<-gsub("des_","", x)
  x<-gsub("avgweight", "Average Weight", x)
  x<-gsub("yearpublished", "Year Published", x)
  x<-gsub("minage", "Min Age", x)
  x<-gsub("playingtime", "Playing Time", x)
  x<-gsub("maxplayers", "Max Players", x)
  x<-gsub("minplayers", "Min Players", x)
  x<-gsub("_", " ", x)

  str_to_title(x)

}

# now make coef plot
glmnet_coefs %>%
  filter(abs(estimate) > .01) %>%
  mutate(term = rename_func(term)) %>%
  ggplot(., aes(x = reorder(term, estimate),
                y=estimate))+
  coord_flip()+
  geom_point()+
  theme_phil()+
  geom_hline(yintercept = 0)+
  xlab("predictor")+
  theme(axis.text.y = element_text(size=6))
                
```

##### Linear Regression with STAN

Let's now fit our a linear model using stan. We'll fit to our training folds in order to estimate our out of sample error - this is slightly computationally taxing and we could do better to use the loo package to estimate the model's performance, but we'll use the training folds to directly compare to our other models.

```{r fit bayesian lienar regression}

# build workflow
set.seed(1999)
stan_lm_workflow<-
  workflow() %>% 
  add_model(stan_lm_mod) %>% 
  add_recipe(recipe_norm_average)

# fit
stan_lm_results <-stan_lm_workflow %>%
  fit_resamples(train_folds,
            control = control_grid(save_pred = TRUE),
            metrics = reg_metrics)

```

We can look at the predictions from our model.

```{r get preds from stan, warning=F, message=F}

# get the resampling results
stan_lm_results %>%
  collect_metrics()

stan_lm_results %>%
  collect_predictions() %>%
  arrange(.row) %>%
  left_join(., baked_train) %>%
  arrange(desc(.pred)) %>%
  ggplot(., aes(x=.pred, y=average))+
  geom_point(alpha=0.25)+
    geom_smooth(formula = y ~ x, method = "loess")+
  coord_cartesian(xlim = c(3.5, 9),
                  ylim = c(3.5, 9))+
  stat_cor(label.x = 3.5,
           p.accuracy = 0.001)+
  geom_abline(slope =1,
              intercept = 0,
              col = "grey60",
              linetype = 'dashed')+
  theme_phil()

```

The biggest utility of using Stan to train our linear model is that we can easily simulate from it to display the uncertainty around our parameter estimates, as well as the uncertainty around our predictions. For this we'll grab the full model.

```{r predict folds, echo=F, warning=F}

set.seed(1999)
stan_fit<-stan_lm_workflow %>%
  fit(games_train) %>%
  extract_fit_parsnip()

```

We can plot the coefficients from this model along with their confidence interval.

```{r get model fit from stan, warning=F, message=F, fig.height=4}

library(tidybayes)
library(broom.mixed)

stan_fit %>%
  tidy(conf.int=T) %>% 
  arrange(desc(estimate)) %>%
  filter(term != '(Intercept)') %>%
  mutate(term = rename_func(term)) %>%
  ggplot(., aes(x=reorder(term,
                          estimate),
                y=estimate,
                ymin = conf.low,
                ymax = conf.high))+
  geom_pointrange(size = 0.1)+
  coord_flip()+
  geom_hline(yintercept = 0)+
  ylab("Estimate")+
  xlab("")+
  theme_phil()+
  theme(axis.text.y = element_text(size = 5))+
  theme(panel.grid.major = element_blank())
  
```

We can also get the predictive interval for each individual predictions, sampling from the posterior.

```{r get predictive interval from stan model, fig.height=4}

set.seed(1999)
baked_train_norm <-recipe_norm_average %>%
  prep(games_train) %>%
  bake(games_train) %>%
  mutate(.row = row_number())

# posterior predict
pred<-stan_fit$fit %>%
  posterior_predict(baked_train_norm)

# temp_dat
temp_dat<-pred %>%
  tidy_draws() %>%
  melt(id.vars = c(".chain",
                   ".iteration",
                   ".draw")) %>%
  mutate(.row = as.integer(variable)) %>%
  left_join(., baked_train_norm %>%
              select(game_id, name, average, .row)) %>%
  filter(game_id %in% samp)
  
# plot each simulation
temp_dat %>%
  ggplot(., aes(x=reorder(name,
                         average),
                y=value)) +
#  geom_density_ridges()+
  geom_point(alpha=0.05,
             col = "grey60",
             size=0.8)+
  coord_flip() +
  theme_phil()+
  geom_point(data = temp_dat,
             aes(x=reorder(name,
                           average),
                 y=average),
             col = "blue")+
  ylab("Simulated Ratings from Model")+
  xlab("")

# get just the predictive interval
t(apply(pred, 2, quantile, probs = c(.05, .1, .9, .95))) %>%
  as_tibble() %>%
  mutate(.row = row_number()) %>%
  left_join(., baked_train_norm %>%
              select(game_id, name, average, .row)) %>%
  filter(game_id %in% samp) %>%
  ggplot(., aes(x=reorder(name,
                         average),
               y=average,
                ymin = `5%`,
               ymax = `95%`)) +
  geom_pointrange()+
#  geom_density_ridges()+
  geom_point(alpha=0.05,
             col = "grey60",
             size=0.8)+
  coord_flip() +
  theme_phil()

rm(temp_dat, pred, stan_pred)


```

We can plot simulated datsets from our model against the observed data

```{r simultae from posterior vs}

# get osterior predictions
stan_pred <- posterior_predict(stan_fit$fit)

samps<-sample(1:nrow(stan_pred),
              50)

# pull from dataset
bayesplot::ppc_dens_overlay(y = stan_fit$fit$y,
                            yrep = stan_pred[samps,])

```

Here we start to run into the biggest issue with using a simple linear model for this data. This is because the Average Rating is a function of both the average rating and user ratings. For predicting new games, we might be better off predicting these two quantities separately and then combining them to produce an estimated Average Rating. More on that later.

##### xgbTree

Let's up the flexibility of the model and turn to our old favorite, gradient boosted trees.

```{r create xgbTree workflow}

# XGBoost model specification
xgbTree_mod <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 500,
    sample_size = tune::tune(),
    min_n = tune::tune(),
    tree_depth = tune::tune()) %>%
  set_engine("xgboost", 
               objective = "reg:squarederror")

```

Now create the tuning grid.

```{r tuning grid for xgbTree}

# parameter speciofication
xgbTree_params <- 
  dials::parameters(
    sample_size(),
    min_n(),
    tree_depth()
  )

# grod
xgbTree_grid <- 
  expand.grid(
    sample_size = c(0.5, 0.75, 0.95),
    min_n = c(5, 15, 30),
    tree_depth = 3
  )

  #             learn_rate = .c
  # dials::grid_max_entropy(
  #   xgbTree_params, 
  #   size = 5
  #)

```

Now create the workflow and tune the model.

```{r tune workflow for xgbTree}

# build workflow
set.seed(1999)
xgbTree_workflow<-
  workflow() %>% 
  add_model(xgbTree_mod) %>% 
  add_recipe(recipe_average)

# tune
xgbTree_results <-
  xgbTree_workflow %>%
  tune_grid(train_folds,
            grid = xgbTree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = reg_metrics)

```

Gather the results as before.

```{r plot results across tuning metrics for xgbTree}

# plot
xgbTree_results %>%
  collect_metrics(summarize=F) %>%
 # select(min_n, tree_depth, sample_size, .metric, mean) %>%
  filter(.metric == 'rmse') %>%
 # filter(sample_size == 0.5) %>%
  mutate(model = "xgbTree") %>%
  ggplot(., aes(x=min_n,
                y=.estimate,
                color  = id,
                dodge = sample_size))+
    geom_point()+
    geom_line()+
    facet_wrap(.metric ~ sample_size)+
  theme_phil()+
  scale_color_viridis_d()


```

Get predictions

```{r xgbtree}

xgbTree_best<-xgbTree_results %>%
  select_best("rmse")

xgbTree_results %>%
  collect_predictions(parameters = xgbTree_best) %>%
  mutate(model = "xgbTree") %>%
  arrange(.row) %>%
  ggplot(., aes(x=.pred,
                y=average))+
  geom_point(alpha=0.5)+
  facet_wrap(model~.)+
  theme_phil()+
  geom_smooth(formula = y ~ x, method = "loess")+
  coord_cartesian(xlim = c(3.5, 9),
                  ylim = c(3.5, 9))+
  stat_cor(label.x = 3.5,
           p.accuracy = 0.001)+
  geom_abline(slope =1,
              intercept = 0,
              col = "grey60",
              linetype = 'dashed')
  
```

look at results

```{r xgbtree pred sample, warning=F, message=F}

# get predictions
xgbTree_results %>%
  collect_predictions(parameters = xgbTree_best) %>%
  left_join(., baked_train) %>%
  arrange(.row) %>%
  select(id, .pred, .row, average, game_id, name) %>%
  filter(game_id %in% samp) %>%
  arrange(desc(.pred)) %>%
  mutate(game_id = as.character(game_id)) %>%
  mutate(model = "xgbTree") %>%
  select(model, game_id, name, average, .pred) %>%
  arrange(desc(average)) %>%
  mutate_if(is.numeric, round, 3) %>%
  flextable() %>%
  autofit() %>%
        bg(j = c('.pred', 'average'),
           bg = col_func)


```


```{r look at the predictions from the boosted trees, warning=F, message=F}

xgbTree_results %>%
  collect_predictions(parameters = xgbTree_best) %>%
  mutate(model = "xgbTree") %>% 
  arrange(.row) %>%
  left_join(., baked_train) %>%
  select(.pred, .row, average, game_id, name, yearpublished) %>%
  filter(yearpublished == 2019) %>%
  arrange(desc(.pred)) %>%
  mutate(pred_rank = row_number()) %>%
  arrange(desc(average)) %>%
  mutate(bgg_rank = row_number()) %>%
  mutate_if(is.numeric, round, 3) %>%
  rename(pred_rating = .pred,
         bgg_rating = average) %>%
  mutate(yearpublished = as.character(yearpublished),
         game_id = as.character(yearpublished)) %>%
  select(yearpublished, game_id, name, pred_rating, bgg_rating, pred_rank, bgg_rank) %>%
  arrange(pred_rank) %>%
  # mutate(diff_rank = pred_rank - bgg_rank) %>%
  # arrange(diff_rank) %>%
  flextable() %>%
  autofit()
  
```

plot of ranks

```{r plot rank vs rank, warning=F, message=F}

xgbTree_results %>%
  collect_predictions(parameters = xgbTree_best) %>%
  sample_n(500) %>%
  mutate(model = "xgbTree") %>%
  select(id, .row, average, .pred, model) %>%
  arrange(average) %>%
  mutate(rank = row_number()) %>%
  melt(id.vars = c("id", ".row", "model", "rank")) %>%
  ggplot(., aes(x=rank,
                y = value,
                color = variable))+
  geom_point(alpha=0.75)+
  theme_phil()+
  scale_color_manual(values = c("deepskyblue1", "navy"))+
  facet_wrap(model~.)+
  ylab("Rating")+
  xlab("Rank")+
  geom_smooth()


```
variable importance

```{r extract xgbTree model}

library(vip)

xgbTree_fit<-xgbTree_workflow %>%
  finalize_workflow(xgbTree_best) %>%
  fit(games_train) %>%
  extract_fit_parsnip()

# variable importance
vip(xgbTree_fit$fit,
    num_features =25,
    all_permutations = T) +
  theme_phil()

```
partial dependence 

```{r pdp for xgbTree, fig.height=4}

# specify tain ojb
train_obj = baked_train %>%
  select(one_of(xgbTree_fit$fit$feature_names))

# select top predictors by importance
top_predictors<-vip::vi(xgbTree_fit$fit) %>%
  slice_max(., order_by = Importance,
            n = 16) %>%
  pull(Variable)
  
library(pdp)

# create a function for partial
partial_func<-function(mod, feature, train_obj) {
  
  var<-enquo(feature)
  var1<-rlang::sym(paste(feature))
  
  foo<-pdp::partial(mod,
                  train = train_obj,
             pred.var = paste(feature),
             center =T,
         #    trim.outliers = T,
             ice=T,
             plot=F,
             type = "regression")
  
  out<-foo %>%
    as_tibble() %>%
    mutate(variable = paste(feature)) %>%
    rename(value = !!var1) %>%
    select(variable, value, yhat, yhat.id)
  
  return(out)
  
  # out %>%
  #   ggplot(., aes(x=value,
  #               y = yhat,
  #               group = yhat.id))+
  #   geom_path(alpha=0.25)+
  #   theme_minimal()+
  #   facet_wrap(variable~.)

}

# loop over top predictors
partials<-foreach(i = 1:length(top_predictors),
        .combine = rbind.data.frame) %do% {
          
          partial_func(mod = xgbTree_fit$fit,
             feature = top_predictors[i],
             train_obj = train_obj)
          
        }

# plot
partials %>%
  #filter(variable == 'number_mechanics' | variable == 'avgweight') %>%
  mutate(variable = rename_func(variable)) %>%
  ggplot(., aes(x=value,
                y = yhat))+
  geom_line(aes(group = yhat.id), alpha=0.05)+
  stat_summary(fun=median,
               geom="line",
               col = "orange",
               alpha = 0.6)+
  facet_wrap(variable~.,
               scales = "free")+
  theme_phil()+
  xlab("Feature Value")+
  ylab("Effect on Average Rating")+
  labs(caption = str_wrap("Centered individal conditional expectation plots for top predictors from xgbTree. Model trained on board games published between 1990 and 2019 prior with at least 200 user ratings.", 125))

rm(partials)

```

Shapley values

```{r start using iml 04, warning=F, message=F}
 
library(iml)
 
# make my own prediction function
predict.function <- function(model, new_observation) {
  predict(model, new_observation)
}

X = train_obj %>% as.data.frame()
y= baked_train$average

# # put ranger into object
# predictor.ranger <- Predictor$new(
#   model = models_down$down_ranger,
#   data = X,
#   y = train_down$NEXT_FOUR_QUARTERS_AT_LEAST_ONE_POLICY,
#   type="prob",
#   class="yes"
#   )
 
# make function for plotting shapley values
shapley_plot<-function(predictor,
                       model_name,
                       train_data,
                       input_id,
                       threshold) {
       
        # require(ModelMetrics)
        # require(gsubfn)
        # require(conflicted)
        # 
    #    conflict_prefer("list", "base")
  
  # use model to define X object with features
  X = train_data %>%
    select(one_of(predictor$model$fit$feature_names))
  
  # look up game id in training set
  record = train_data %>%
    filter(game_id == input_id) %>%
    pull(.row)
  
 # # look up the game
  game_name<-train_data %>%
    filter(.row == record) %>%
    pull(name)
  
  id<-train_data %>%
    filter(.row == record) %>%
    pull(game_id)

  # # temp obj for the predictor
  temp<-predictor
  
  # get shapley values
  shapley <- Shapley$new(temp,
                         sample.size = 250,
                          x.interest = X[record, ])
  
  # actual vs
    average = paste("Average Game Prediction:", round(shapley$y.hat.average, 2))
    actual =  paste("Game Prediction:", round(shapley$y.hat.interest, 2))

    # plot
  plot_shap<-shapley$results %>%
                separate(feature.value, into=c("Feature", "Value"), sep="=") %>%
                mutate(Feature = rename_func(Feature),
                       Value = round(as.numeric(Value), 2)) %>%
                mutate(Feature.Value = paste(Feature, Value, sep="=")) %>%
                filter(abs(phi) > threshold) %>%
    mutate(model = paste(model_name)) %>%
                ggplot(aes(x = reorder(Feature.Value, -phi), y = phi, fill = phi)) +
        geom_hline(yintercept = 0,
               alpha = 0.5)+
                geom_bar(stat = "identity", alpha = 0.95) +

                coord_flip() +
                guides(fill=F)+
                scale_fill_gradient2_tableau(limits=c(-.02,0.02), oob = scales::squish)+
                theme_phil()+
                ggtitle(paste(
                  paste("Game: ", game_name, sep=""),
                  paste("ID: ", id, sep=""), 
                  sep = "\n"),
                        subtitle = paste(average,actual, sep = "\n"))+
                theme(plot.title = element_text(size=12),
                      axis.text.y = element_text(size=8))+
                xlab("")+
                ylab("Feature Contribution to Prediction")+
    facet_wrap(model~.)

        rm(actual, average, temp)

        return(plot_shap)

}

# put xgbTree into predictor object
predictor.xgbTree <- Predictor$new(
  model = xgbTree_fit,
  data = X,
  y = y
  )

#  
# # put glmnet into object
# predictor.glmnet <- Predictor$new(
#   model = models_down$down_glmnet_oneSE,
#   data = X,
#   y = baked_train$
#   type="prob",
#   class="yes"
#   )
 
```
 
We can then look at shapley values for a sample of games to see how the model arrived at its predictions. These will be slightly different than the predictions we observed for our games earlier, as these are using the model that was trained on the full dataset rather than the cross validated predictions.

```{r illustrate shapley values for xgbTree, warning=F, message=F, fig.height=4}
 
samp_games = c(521, 115746, 822, 204583, 15062, 84876, 192455)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[1],
             threshold=0.01)
  
shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[2],
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[3],
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[4],
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[5],
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[6],
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = samp_games[7],
             threshold=0.01)

```


```{r fig.height=4}

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = 174430,
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = 167355,
             threshold=0.01)

shapley_plot(predictor.xgbTree,
             model_name = "xgbTree",
             train_data = baked_train,
             input_id = 1406,
             threshold=0.01)

```
Save

```{r save the models}

models = lapply(ls(pattern="fit_"), get)
model_names = lapply(ls(pattern="fit_"), get)
save(models, file = paste("game_predictions_models/average", Sys.Date(), ".csv", sep=""))

```


## Evaluation

Assemble our test set

```{r assemble test set}

# combine all
games_test<-active_games %>%
  select(timestamp, game_id, name, average, baverage, usersrated) %>%
  left_join(., games_info %>% # join game info
              select(game_id, yearpublished, avgweight, minage, minplayers, maxplayers, playingtime),
            by = c("game_id")) %>%
  filter(yearpublished >= 2020) %>% # use games after 2020 as our test
  left_join(., game_categories %>% # join categories
              mutate(category = gsub("\\)", "", gsub("\\(", "", category))) %>%
              mutate(category = tolower(paste("cat", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", category))), sep="_"))) %>%
              mutate(has_category = 1) %>%
              select(-category_id) %>%
              pivot_wider(names_from = c("category"),
                          values_from = c("has_category"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
            by = c("game_id")) %>%
  left_join(., game_mechanics %>% # join mechanics
              mutate(mechanic = tolower(paste("mech", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", mechanic))), sep="_"))) %>%
              mutate(has_mechanic = 1) %>%
              select(-mechanic_id) %>%
              pivot_wider(names_from = c("mechanic"),
                          values_from = c("has_mechanic"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
            by = c("game_id")) %>%
      left_join(., game_designers %>% # join designers
              mutate(designer = gsub("\\)", "", gsub("\\(", "", designer))) %>%
              mutate(designer = tolower(paste("des", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", designer))), sep="_"))) %>%
              mutate(has_designer = 1) %>%
              select(-designer_id) %>%
              pivot_wider(names_from = c("designer"),
                          values_from = c("has_designer"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
            by = c("game_id")) %>%
        left_join(., game_publishers %>% # join publishers
                          filter(publisher_id %in% publisher_list) %>%
              mutate(publisher = gsub("\\)", "", gsub("\\(", "", publisher))) %>%
              mutate(publisher = tolower(paste("pub", gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", publisher))), sep="_"))) %>%
              mutate(has_publisher = 1) %>%
              select(-publisher_id) %>%
              pivot_wider(names_from = c("publisher"),
                          values_from = c("has_publisher"),
                          id_cols = c("game_id"),
                          names_sep = "_",
                          values_fn = min,
                          values_fill = 0),
              by = c("game_id"))


```

Now predict games published starting in 2020.

```{r eval on 2020}

# set color functions
col_func<- function(x) {
  
  breaks<-quantile(x, probs = seq(0, 1, by=.1), na.rm=T) %>% as.vector()
#  breaks = weight_deciles
  colorRamp=colorRampPalette(c("red", "white", "deepskyblue1"))
  col_palette <- colorRamp(length(breaks))
  mycut <- cut(x, 
    breaks = breaks,
    include.lowest = TRUE, 
    right=T,
    label = FALSE)
  col_palette[mycut]
  
}

# stan
stan_pred<-stan_lm_workflow %>%
  fit(games_train) %>%
  predict(games_test)


# xgbTree
xgbTree_pred<-xgbTree_workflow %>%
  finalize_workflow(parameters = xgbTree_best) %>%
  fit(games_train) %>%
  predict(games_test)

```

### 2020

Combine and look

```{r eval 2020 preds}

preds_2020<-data.frame("stan_lm" = stan_pred$.pred,
           "xgbTree" = xgbTree_pred$.pred) %>%
  cbind.data.frame(games_test) %>%
  arrange(desc(stan_lm)) %>%
  mutate(yearpublished = as.character(yearpublished),
         game_id = as.character(game_id)) %>%
  select(yearpublished, 
         game_id, 
         name, 
         average,
         stan_lm, 
         xgbTree) %>%
  filter(yearpublished == 2020)

# table
preds_2020 %>%
  mutate_if(is.numeric, round, 2) %>%
  arrange(desc(stan_lm)) %>%
  flextable() %>%
  autofit() %>%
  bg(j = c('average', 'stan_lm', 'xgbTree'),
  bg = col_func)

# metrics
preds_2020 %>%
  melt(id.vars = c("yearpublished", "game_id", "name", "average")) %>% 
  group_by(yearpublished, variable) %>%
  reg_metrics(truth = average,
              estimate = value) 
  # ggplot(., aes(y=reorder(variable, .estimate),
  #               x=.estimate))+
  # geom_point()+
  # facet_wrap(yearpublished+.metric~., scales="free",
  #            ncol = 2)+
  # theme_phil()+
  # ylab("Model")

# plot vs average
preds_2020 %>%
  melt(id.vars = c("yearpublished", "game_id", "name", "average")) %>%
  ggplot(., aes(x=average,
                y=value))+
  geom_point(alpha=0.5)+
  facet_wrap(yearpublished+variable~.)+
  theme_phil()+
  geom_smooth(method = 'loess', formula = 'y ~ x')+
  stat_cor(p.accuracy=.01)+
  geom_abline(slope=1, intercept = 0)


# xgbtree and stan
preds_2020 %>%
  ggplot(., aes(x=xgbTree,
                label = name,
             #   color = average,
                y=stan_lm))+
  geom_text(check_overlap =T)+
  geom_point(alpha=0.8)+
 # stat_cor(p.accuracy=0.01)+
  theme_phil()+
  theme(legend.title = element_text())

# # ranger and xgbtree
# preds_2020 %>%
#   ggplot(., aes(x=xgbTree,
#                 label = name,
#              #   color = average,
#                 y=ranger))+
#   geom_text(check_overlap =T)+
#   geom_point(alpha=0.8)+
#  # stat_cor(p.accuracy=0.01)+
#   theme_phil()+
#   theme(legend.title = element_text())
# 
# # xgbtree and knn
# preds_2020 %>%
#   ggplot(., aes(x=xgbTree,
#                 label = name,
#              #   color = average,
#                 y=knn))+
#   geom_text(check_overlap =T)+
#   geom_point(alpha=0.8)+
#  # stat_cor(p.accuracy=0.01)+
#   theme_phil()+
#   theme(legend.title = element_text())
  # coord_cartesian(xlim=c(5,8.5),
  #                 ylim =c(5, 8.5))+
  # scale_color_gradient2_tableau(limits=c(6,6.5),
  #                               oob = scales::squish)+
  # guides(color = guide_colorbar(barwidth=10,
  #                               barheight=0.5,
  #                               title.position = 'top',
  #                               title = "Average Rating"))


```


### 2021

```{r eval on 2021}

# set color functions
col_func<- function(x) {
  
  breaks<-quantile(x, probs = seq(0, 1, by=.1), na.rm=T) %>% as.vector()
#  breaks = weight_deciles
  colorRamp=colorRampPalette(c("red", "white", "deepskyblue1"))
  col_palette <- colorRamp(length(breaks))
  mycut <- cut(x, 
    breaks = breaks,
    include.lowest = TRUE, 
    right=T,
    label = FALSE)
  col_palette[mycut]
  
}

preds_2021<-data.frame("stan_lm" = stan_pred$.pred,
           "xgbTree" = xgbTree_pred$.pred) %>%
  cbind.data.frame(games_test) %>%
  arrange(desc(stan_lm)) %>%
  mutate(yearpublished = as.character(yearpublished),
         game_id = as.character(game_id)) %>%
  select(yearpublished, 
         game_id, 
         name, 
         average,
         stan_lm, 
         xgbTree) %>%
  filter(yearpublished == 2021)

# table
preds_2021 %>%
  mutate_if(is.numeric, round, 2) %>%
  arrange(desc(stan_lm)) %>%
  flextable() %>%
  autofit() %>%
  bg(j = c('average', 'stan_lm', 'xgbTree'),
  bg = col_func)

# metrics
preds_2021 %>%
  melt(id.vars = c("yearpublished", "game_id", "name", "average")) %>% 
  group_by(yearpublished, variable) %>%
  reg_metrics(truth = average,
              estimate = value) 
  # ggplot(., aes(y=reorder(variable, .estimate),
  #               x=.estimate))+
  # geom_point()+
  # facet_wrap(yearpublished+.metric~., scales="free",
  #            ncol = 2)+
  # theme_phil()+
  # ylab("Model")

# plot vs average
preds_2021 %>%
  melt(id.vars = c("yearpublished", "game_id", "name", "average")) %>%
  ggplot(., aes(x=average,
                y=value))+
  geom_point(alpha=0.5)+
  facet_wrap(yearpublished+variable~.)+
  theme_phil()+
  geom_smooth(method = 'loess', formula = 'y ~ x')+
  stat_cor(p.accuracy=.01)+
  geom_abline(slope=1, intercept = 0)


# xgbtree and stan
preds_2021 %>%
  ggplot(., aes(x=xgbTree,
                label = name,
             #   color = average,
                y=stan_lm))+
  geom_text(check_overlap =T)+
  geom_point(alpha=0.8)+
 # stat_cor(p.accuracy=0.01)+
  theme_phil()+
  theme(legend.title = element_text())

# # ranger and xgbtree
# preds_2020 %>%
#   ggplot(., aes(x=xgbTree,
#                 label = name,
#              #   color = average,
#                 y=ranger))+
#   geom_text(check_overlap =T)+
#   geom_point(alpha=0.8)+
#  # stat_cor(p.accuracy=0.01)+
#   theme_phil()+
#   theme(legend.title = element_text())
# 
# # xgbtree and knn
# preds_2020 %>%
#   ggplot(., aes(x=xgbTree,
#                 label = name,
#              #   color = average,
#                 y=knn))+
#   geom_text(check_overlap =T)+
#   geom_point(alpha=0.8)+
#  # stat_cor(p.accuracy=0.01)+
#   theme_phil()+
#   theme(legend.title = element_text())
  # coord_cartesian(xlim=c(5,8.5),
  #                 ylim =c(5, 8.5))+
  # scale_color_gradient2_tableau(limits=c(6,6.5),
  #                               oob = scales::squish)+
  # guides(color = guide_colorbar(barwidth=10,
  #                               barheight=0.5,
  #                               title.position = 'top',
  #                               title = "Average Rating"))

```


### 2022

```{r eval on 2022}

# set color functions
col_func<- function(x) {
  
  breaks<-quantile(x, probs = seq(0, 1, by=.1), na.rm=T) %>% as.vector()
#  breaks = weight_deciles
  colorRamp=colorRampPalette(c("red", "white", "deepskyblue1"))
  col_palette <- colorRamp(length(breaks))
  mycut <- cut(x, 
    breaks = breaks,
    include.lowest = TRUE, 
    right=T,
    label = FALSE)
  col_palette[mycut]
  
}

preds_2022<-data.frame("stan_lm" = stan_pred$.pred,
           "xgbTree" = xgbTree_pred$.pred) %>%
  cbind.data.frame(games_test) %>%
  arrange(desc(stan_lm)) %>%
  mutate(yearpublished = as.character(yearpublished),
         game_id = as.character(game_id)) %>%
  select(yearpublished, 
         game_id, 
         name, 
         average,
         stan_lm, 
         xgbTree) %>%
  filter(yearpublished == 2022)

# table
preds_2022 %>%
  select(-average) %>%
  mutate_if(is.numeric, round, 2) %>%
  arrange(desc(stan_lm)) %>%
  flextable() %>%
  autofit() %>%
  bg(j = c('stan_lm', 'xgbTree'),
  bg = col_func)


# xgbtree and stan
preds_2022 %>%
  ggplot(., aes(x=xgbTree,
                label = name,
             #   color = average,
                y=stan_lm))+
  geom_text(check_overlap =T)+
  geom_point(alpha=0.8)+
 # stat_cor(p.accuracy=0.01)+
  theme_phil()+
  theme(legend.title = element_text())

# # ranger and xgbtree
# preds_2020 %>%
#   ggplot(., aes(x=xgbTree,
#                 label = name,
#              #   color = average,
#                 y=ranger))+
#   geom_text(check_overlap =T)+
#   geom_point(alpha=0.8)+
#  # stat_cor(p.accuracy=0.01)+
#   theme_phil()+
#   theme(legend.title = element_text())
# 
# # xgbtree and knn
# preds_2020 %>%
#   ggplot(., aes(x=xgbTree,
#                 label = name,
#              #   color = average,
#                 y=knn))+
#   geom_text(check_overlap =T)+
#   geom_point(alpha=0.8)+
#  # stat_cor(p.accuracy=0.01)+
#   theme_phil()+
#   theme(legend.title = element_text())
  # coord_cartesian(xlim=c(5,8.5),
  #                 ylim =c(5, 8.5))+
  # scale_color_gradient2_tableau(limits=c(6,6.5),
  #                               oob = scales::squish)+
  # guides(color = guide_colorbar(barwidth=10,
  #                               barheight=0.5,
  #                               title.position = 'top',
  #                               title = "Average Rating"))

```

Predictions

```{r write out as csv}

preds_out<-bind_rows(preds_2020,
                     preds_2021,
                     preds_2022) %>%
  mutate(date = Sys.Date()) %>%
  select(date, everything())

fwrite(preds_out, file = paste("game_predictions_data/average_", Sys.Date(),".csv", sep=""))

```


Models

```{r save models}

model_names <-ls(pattern = "_fit")
models = lapply(ls(pattern = "_fit"), get)
names(models)=model_names
save(models, file = paste("game_predictions_models/average_", Sys.Date(), ".csv", sep=""))

```

